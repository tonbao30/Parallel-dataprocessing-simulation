{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this to install the library\n",
    "# !pip3 install pygeohash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the libraries\n",
    "from time import sleep\n",
    "from kafka import KafkaConsumer\n",
    "import datetime as dt\n",
    "import pygeohash as pgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuctions to check the location based on the geo hash (precision =5)\n",
    "#function to check location between 2 data\n",
    "def close_location (data1,data2):\n",
    "    print(\"checking location...of sender\",data1.get(\"id\"),\" and  sender\" , data2.get(\"id\"))\n",
    "\n",
    "    #with the precision =5 , we find the location that close together with the radius around 2.4km\n",
    "    if data1.get(\"geohash\")== data2.get(\"geohash\"): \n",
    "        print(\"=>>>>>sender\",str(data1.get(\"id\")),\"location near   \",  \"sender\",str(data2.get(\"id\")),\"location\")\n",
    "    else:\n",
    "        print('>>>not close together<<<')\n",
    "        \n",
    "#function to check location between the joined data and another data (e.g hotspot data)\n",
    "def close_location_2 (data1,data2): \n",
    "    print(\"checking location...of joined data id:\",data1.get(\"id\"),\" and  sender\" , data2.get(\"id\"))\n",
    "    \n",
    "    #with the precision =5 , we find the location that close together with the radius 2.4km\n",
    "    if data1.get(\"geohash\")== data2.get(\"geohash\"): \n",
    "        print(\"=>>>> location\",str(data1.get(\"geohash\")),\"location near   \",  str(data2.get(\"geohash\")),\"location\")\n",
    "    else:\n",
    "        print('>>>not close together<<<')\n",
    "\n",
    "\n",
    "# check location of 2 climate data stored in the list\n",
    "def close_location_in_list(a_list):\n",
    "    print('check 2 climate location data')\n",
    "    data_1 = a_list[0]\n",
    "    data_2 = a_list[1]\n",
    "    close_location (data_1,data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary function to handle the average and join of the json file\n",
    "#function to merge satellite data\n",
    "def merge_sat(data1,data2):\n",
    "    result ={}\n",
    "    \n",
    "    result[\"_id\"] = data1.get(\"_id\") # take satellite _id ,we will store this joined data to the hotspot collection\n",
    "    result[\"created_time\"] = data1.get(\"created_time\")\n",
    "    \n",
    "    \n",
    "    #average the result of the location\n",
    "    result['surface_temperature_celsius'] = (float(data1.get(\"surface_temperature_celsius\"))+float(data2.get(\"surface_temperature_celsius\")))/2    \n",
    "    result[\"confidence\"] = (float(data1.get(\"confidence\"))+float(data2.get(\"confidence\")))/2\n",
    "    \n",
    "    #reassign the location like the initial data structure\n",
    "    result['geohash'] = data2.get('geohash')\n",
    "    result[\"location\"] = data1.get(\"location\")\n",
    "\n",
    "        \n",
    "    return result\n",
    "\n",
    "# function to join climate data and satellite data\n",
    "def join_data_cli_sat(climData,satData):\n",
    "    result={}\n",
    "\n",
    "    #get location and id of the join data\n",
    "    result[\"_id\"] = climData.get(\"_id\") # take climate _id ,we will store this joined data to the climate collection\n",
    "    result['geohash'] = climData.get('geohash')\n",
    "    result[\"location\"] = climData.get(\"location\")\n",
    "    result[\"created_time\"] = climData.get(\"created_time\")\n",
    "    \n",
    "\n",
    "    #get climate data\n",
    "    result[\"air_temperature_celsius\"] = climData.get(\"air_temperature_celsius\")\n",
    "    result[\"relative_humidity\"] = climData.get(\"relative_humidity\")\n",
    "    result[\"max_wind_speed\"] = climData.get(\"max_wind_speed\")\n",
    "    result[\"windspeed_knots\"] = climData.get(\"windspeed_knots\")\n",
    "    result[\"precipitation\"] = climData.get(\"precipitation\")\n",
    "   \n",
    "    #get satellite data\n",
    "    result[\"surface_temperature_celsius\"] = satData.get(\"surface_temperature_celsius\")\n",
    "    result[\"confidence\"] = satData.get(\"confidence\")\n",
    "    result[\"hotspots\"] = satData.get(\"_id\") #reference to the hotspot data like in the task A_B\n",
    "        \n",
    "    return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:45:20\n",
      "-------------------------------------------\n",
      "sender_2\n",
      "sender_3\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:45:30\n",
      "-------------------------------------------\n",
      "sender_3\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:45:40\n",
      "-------------------------------------------\n",
      "sender_2\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:45:50\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_2\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:46:00\n",
      "-------------------------------------------\n",
      "sender_3\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:46:10\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_2\n",
      "sender_3\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:46:20\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:46:30\n",
      "-------------------------------------------\n",
      "sender_2\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:46:40\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_3\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:46:50\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:47:00\n",
      "-------------------------------------------\n",
      "sender_2\n",
      "sender_3\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:47:10\n",
      "-------------------------------------------\n",
      "sender_2\n",
      "sender_1\n",
      "sender_3\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:47:20\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:47:30\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_3\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:47:40\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_2\n",
      "sender_1\n",
      "sender_3\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:47:50\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_1\n",
      "sender_3\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:48:00\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:48:10\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_2\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:48:20\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_3\n",
      "sender_1\n",
      "sender_2\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:48:30\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:48:40\n",
      "-------------------------------------------\n",
      "sender_1\n",
      "sender_1\n",
      "sender_3\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3df78dc53f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;31m# ssc.start()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run stream for 20 mins just to get the data for visualisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;31m# # ssc.awaitTermination()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:48:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:49:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:49:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:49:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:49:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:49:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:49:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:50:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:50:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:50:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:50:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-24 17:50:40\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    \n",
    "    # MongoDB design\n",
    "    sat_col = db.hotspot #to store satellite data and joined satellite data \n",
    "     \n",
    "    # to store the join between climate and satellite\n",
    "    clim_col = db.climate #to store the climate data\n",
    "    \n",
    "    #list of senders per iter\n",
    "    sender = []\n",
    "    \n",
    "    #variable to store the data from 3 unique senders per iter\n",
    "    climList = []\n",
    "    satData_2 = {}\n",
    "    satData_3 = {}\n",
    "#####################################  PARSING THE DATA FROM SENDERS  PER ITER###########################################\n",
    "    for record in iter: \n",
    "            sender.append(record[0])\n",
    "            data_id = json.loads(record[1])\n",
    "            data = data_id.get('data')\n",
    "            \n",
    "\n",
    "\n",
    "            if record[0] == \"sender_2\" : #parse AQUA satelite data\n",
    "\n",
    "                \n",
    "                #main data\n",
    "                #add \"AQUA\" string to the \"_id\" to handle the case when 2 satellite data come at the same time\n",
    "                #to make sure the incomming data from AQUA at a specific time is unique\n",
    "                satData_2[\"_id\"] = \"AQUA\" +str(dt.datetime.strptime(str(data_id.get(\"created_time\")), \"%Y-%m-%dT%H:%M:%S\"))\n",
    "                satData_2[\"id\"] = data_id.get(\"sender_id\") #unique sender_id\n",
    "                \n",
    "                #use datetime as ISO format for readable in mongoDB\n",
    "                satData_2[\"created_time\"] = dt.datetime.strptime(str(data_id.get(\"created_time\")), \"%Y-%m-%dT%H:%M:%S\")\n",
    "                \n",
    "                # parse other data\n",
    "                satData_2[\"location\"] = {\"latitude\" : float(data.get(\"lat\")), \"longitude\" : float(data.get(\"lon\"))}\n",
    "                satData_2[\"surface_temperature_celsius\"] = float(data.get(\"surface_temp\"))\n",
    "                satData_2[\"confidence\"] = float(data.get(\"confidence\"))\n",
    "                geohash = pgh.encode(float(data.get(\"lat\")),float(data.get(\"lon\")),precision=5)            \n",
    "                satData_2[\"geohash\"] = geohash #unique_location               \n",
    "\n",
    "\n",
    "\n",
    "            if record[0] == \"sender_3\": #parse TERRA satelite data\n",
    "\n",
    "                #main data\n",
    "                #add \"TERRA\" string to the \"_id\" to handle the case when 2 satellite data come at the same time\n",
    "                #to make sure the incomming data for TERRA at a specific time is unique\n",
    "                satData_3[\"_id\"] = \"TERRA\" +str(dt.datetime.strptime(str(data_id.get(\"created_time\")), \"%Y-%m-%dT%H:%M:%S\"))\n",
    "                satData_3[\"id\"] = data_id.get(\"sender_id\") #unique sender_id\n",
    "                \n",
    "                #use datetime as ISO format for readable in mongoDB\n",
    "                satData_3[\"created_time\"] = dt.datetime.strptime(str(data_id.get(\"created_time\")), \"%Y-%m-%dT%H:%M:%S\")\n",
    "                # parse other data\n",
    "                satData_3[\"location\"] = {\"latitude\" : float(data.get(\"lat\")), \"longitude\" : float(data.get(\"lon\"))}\n",
    "                satData_3[\"surface_temperature_celsius\"] = float(data.get(\"surface_temp\"))\n",
    "                satData_3[\"confidence\"] = float(data.get(\"confidence\"))\n",
    "                geohash = pgh.encode(float(data.get(\"lat\")),float(data.get(\"lon\")),precision=5)               \n",
    "                satData_3[\"geohash\"] = geohash #unique_location     \n",
    "\n",
    "\n",
    "\n",
    "            if record[0] == \"sender_1\": #parse climate data\n",
    "                climData = {}\n",
    "\n",
    "                \n",
    "                \n",
    "                #main data\n",
    "                #add \"CLIM\" string to the \"_id\" to handle to make sure the incomming data for \n",
    "                #climate at a specific time is unique\n",
    "                climData[\"_id\"] = \"CLIM\" + str(dt.datetime.strptime(str(data_id.get(\"created_time\")), \"%Y-%m-%dT%H:%M:%S\"))\n",
    "                climData[\"id\"] = data_id.get(\"sender_id\") #unique sender_id\n",
    "                \n",
    "                #use datetime as ISO format for readable in mongoDB\n",
    "                climData[\"created_time\"] = dt.datetime.strptime(str(data_id.get(\"created_time\")), \"%Y-%m-%dT%H:%M:%S\")\n",
    "                climData[\"location\"] = {\"latitude\" : float(data.get(\"lat\")), \"longitude\" :  float(data.get(\"lon\"))}\n",
    "\n",
    "                \n",
    "                climData[\"air_temperature_celsius\"] = float(data.get(\"air_temp\"))\n",
    "                climData[\"relative_humidity\"] = float(data.get(\"relative_humid\"))\n",
    "                climData[\"max_wind_speed\"] = float(data.get(\"max_wind_speed\"))\n",
    "                climData[\"windspeed_knots\"] = float(data.get(\"windspeed\"))\n",
    "                climData[\"precipitation\"] = data.get(\"prep\")\n",
    "                geohash = pgh.encode(float(data.get(\"lat\")),float(data.get(\"lon\")),precision=5) \n",
    "                climData[\"geohash\"] = geohash\n",
    "                climList.append(climData)\n",
    "\n",
    "    uniq_sender_id = set(sender) #check unique sender for each iter\n",
    "\n",
    "################################ PERFOMING JOIN AND CHECK LOCATION THEN PUSH TO MONGODB ##################################\n",
    "\n",
    "####################### Received only from unique one sender\n",
    "    \n",
    "    #for climate data, there will be the case with on 2 streams of climate data go throught the app\n",
    "    if len(uniq_sender_id) == 1 and \"sender_1\" in uniq_sender_id:#store to climate data to mongoDB\n",
    "        print(\"---------------------received CLIMATE data------------------------\")\n",
    "        try:\n",
    "            #find close location in climate data and print out\n",
    "            if len(climList) > 1:\n",
    "                #check 2 climate location data\n",
    "                close_location_in_list(climList)\n",
    "                \n",
    "            for data in climList:\n",
    "                clim_col.insert(data)\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    \n",
    "    # if there is one satellite data (AQUA), there will be no case with 2 same satelite data\n",
    "    if len(uniq_sender_id) == 1 and \"sender_2\" in uniq_sender_id:#store to climate data to mongoDB\n",
    "        print(\"---------------------received AQUA data------------------------\")\n",
    "        try:\n",
    "\n",
    "            sat_col.insert(satData_2)\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "            \n",
    "    # if there is one satellite data (TERRA) , there will be no case with 2 same satelite data\n",
    "    if len(uniq_sender_id) == 1 and \"sender_3\" in uniq_sender_id:#store to climate data to mongoDB\n",
    "        print(\"---------------------received TERRA data------------------------\")\n",
    "        try:\n",
    "\n",
    "            sat_col.insert(satData_3)\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    \n",
    "########################## Received  from 2 unique  senders\n",
    "\n",
    "    elif len(sender) == 2 and len(uniq_sender_id) == 2:\n",
    "        print(\"---------------------received 2 streams------------------------\")\n",
    "        #will have 1 case, because there will be at least 1 climate data \n",
    "        #if the consummer received 2, that will be the climat data and one sat data\n",
    "        #or 2 climate data because we assume that there is at least 1 climate data in the stream\n",
    "        \n",
    "       \n",
    "        try:\n",
    "\n",
    "            for climate in climList:\n",
    "\n",
    "\n",
    "                if len(satData_3)!=0:\n",
    "                    \n",
    "                    #check location\n",
    "                    close_location(climate,satData_3)\n",
    "\n",
    "                    #check lat lon first!!!\n",
    "                    print('---checking TERRA and Climate location---')\n",
    "                    if satData_3[\"location\"] == climate[\"location\"]:\n",
    "                        print('joining....')\n",
    "                        join_cli_sat = join_data_cli_sat(climate,satData_3)\n",
    "                        clim_col.insert(join_cli_sat)\n",
    "                        sat_col.insert(satData_3)\n",
    "                    else:\n",
    "                        print('no join')\n",
    "                        sat_col.insert(satData_3)\n",
    "                        clim_col.insert(climate)\n",
    "\n",
    "                elif len(satData_2)!=0:\n",
    "                    #check close location\n",
    "                    close_location(climate,satData_2)\n",
    "\n",
    "                    print('---checking AQUA and Climate location---')\n",
    "                    #check lat lon first!!!\n",
    "                    if satData_2[\"location\"] == climate[\"location\"]:\n",
    "                        print('joining....')\n",
    "                        join_cli_sat = join_data_cli_sat(climate,satData_2)\n",
    "                        clim_col.insert(join_cli_sat)\n",
    "                        sat_col.insert(satData_2)\n",
    "                    else:\n",
    "                        print('no join')\n",
    "                        sat_col.insert(satData_2)\n",
    "                        clim_col.insert(climate)\n",
    "                else: #received only 2 climate data\n",
    "\n",
    "                    print('received 2 climate data')\n",
    "                    clim_col.insert(climate)\n",
    "\n",
    "            # if we received 2 sattelite data only (rare case, we ran out of climate data)\n",
    "            if len(climList) == 0:\n",
    "                if len(satData_3)!=0 and len(satData_2)!=0:\n",
    "                    #check location\n",
    "                    close_location(satData_3,satData_2)\n",
    "                    print('---checking AQUA and TERRA location---')\n",
    "                    if satData_2[\"location\"] == satData_3[\"location\"]:\n",
    "                        print('joining....')\n",
    "                        sat_data = merge_sat(satData_2,satData_3)\n",
    "\n",
    "                        #insert the data into the mongo with handling the exceptions : duplicate\n",
    "                        sat_col.insert(sat_data)\n",
    "                        \n",
    "                    else:\n",
    "                        sat_col.update(satData_3, satData_3, upsert=True)\n",
    "                        sat_col.update(satData_2, satData_2, upsert=True)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex))) #exception will occur with empty satelite data\n",
    "\n",
    "            \n",
    "#########################################################Received 3 stream\n",
    "########################## Received  from 2 unique  sender            \n",
    "\n",
    "#we assume that there is at least 1 climate data in the stream , so if we have 3 streams of data\n",
    "# there will be 2 climate data and 1 satelite data because the app process 10 secs batch\n",
    "# if received 3 streams, there will be 2 climate data and 1 satellite data\n",
    "\n",
    "    if len(sender) == 3: \n",
    "        print(\"---------------------received 3 streams------------------------\")\n",
    "        try:\n",
    "            \n",
    "            if len(climList) > 1:\n",
    "                #check 2 climate location data\n",
    "                close_location_in_list(climList)\n",
    "\n",
    "            for climate2 in climList:\n",
    "\n",
    "                if len(satData_3)!=0:\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    #check location\n",
    "                    close_location(climate2,satData_3)\n",
    "            \n",
    "                    print('---checking TERRA and Climate location---')\n",
    "                    if satData_3[\"location\"] == climate2[\"location\"]:\n",
    "                        print('joining....')\n",
    "\n",
    "                        join_data = join_data_cli_sat(climate2,satData_3)\n",
    "                        clim_col.insert(join_data)\n",
    "                        sat_col.update(satData_3, satData_3, upsert=True)\n",
    "                        \n",
    "\n",
    "                    else:\n",
    "                        print('no join')\n",
    "\n",
    "                        clim_col.insert(climate2)\n",
    "                        \n",
    "                        #insert the data into the mongo with handling the exceptions : duplicate\n",
    "                        sat_col.update(satData_3, satData_3, upsert=True)\n",
    "\n",
    "\n",
    "                elif len(satData_2)!=0:\n",
    "                    \n",
    "                    \n",
    "                    #check location\n",
    "                    close_location(climate2,satData_2)\n",
    "                    \n",
    "                    print('---checking AQUA and Climate location---')\n",
    "                    \n",
    "                    if satData_2[\"location\"] == climate2[\"location\"]:\n",
    "                        print('joining....')\n",
    "                        \n",
    "                        join_data = join_data_cli_sat(climate2,satData_2)\n",
    "                        clim_col.insert(join_data)\n",
    "                        sat_col.update(satData_2, satData_2, upsert=True)\n",
    "                        \n",
    "                    else:\n",
    "                        print('no join')\n",
    "                       \n",
    "                        clim_col.insert(climate2)\n",
    "                        #insert the data into the mongo with handling the exceptions : duplicate\n",
    "                        sat_col.update(satData_2, satData_2, upsert=True)\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "            \n",
    " ########################################Received 4 streams of data#################################   \n",
    "# There will be 2 climate data and 2 satellite data from AQUA and TERRA\n",
    "\n",
    "    elif len(sender) ==4 : # 4 will have 2 climate data and 2 sat data\n",
    "        print(\"---------------------received 4 streams------------------------\")\n",
    "        try:\n",
    "            \n",
    "            if len(climList) > 1:\n",
    "                #check 2 climate location data\n",
    "                close_location_in_list(climList)\n",
    "\n",
    "            for climate2 in climList:\n",
    "                print('---checking AQUA , TERRA and Climate location---')\n",
    "                #location sat2=sat3=climate\n",
    "                if (satData_2[\"location\"] == satData_3[\"location\"])\\\n",
    "                and  (satData_2[\"location\"] == climate2[\"location\"]):\n",
    "                    print('joining....')\n",
    "                    \n",
    "                    #join 2 satellite data\n",
    "                    sat_data = merge_sat(satData_2,satData_3)\n",
    "                    sat_col.update(sat_data, sat_data, upsert=True)\n",
    "                    \n",
    "                    #join with the climate file\n",
    "                    final_data = join_data_cli_sat(climate2,sat_data)\n",
    "                    clim_col.insert(final_data)\n",
    "                    \n",
    "                \n",
    "                #location sat2=sat3\n",
    "                elif (satData_2[\"location\"] == satData_3[\"location\"])\\\n",
    "                and (satData_2[\"location\"] != climate2[\"location\"]):\n",
    "                    print('joining....')\n",
    "                    sat_data = merge_sat(satData_2,satData_3)\n",
    "                    \n",
    "                    #insert the data into the mongo with handling the exceptions : duplicate\n",
    "                    sat_col.update(sat_data, sat_data, upsert=True)\n",
    "                    clim_col.insert(climate2)\n",
    "                    \n",
    "                    #check location\n",
    "                    close_location_2(sat_data,climate2)\n",
    "                \n",
    "                #location sat2=climate \n",
    "                elif (satData_2[\"location\"] != satData_3[\"location\"])\\\n",
    "                and (satData_2[\"location\"] == climate2[\"location\"]):\n",
    "                    print('joining....')\n",
    "                   \n",
    "                    join_data = join_data_cli_sat(climate2,satData_2)\n",
    "                    clim_col.insert(join_data)\n",
    "                    \n",
    "                    #insert the data into the mongo with handling the exceptions : duplicate\n",
    "                    sat_col.update(satData_3, satData_3, upsert=True)\n",
    "                    sat_col.update(satData_2, satData_2, upsert=True)\n",
    "#\n",
    "                    #check location\n",
    "                    close_location_2(join_data,satData_3)\n",
    "                \n",
    "                #location sat3 =climate\n",
    "                elif (satData_2[\"location\"] != satData_3[\"location\"])\\\n",
    "                and (satData_3[\"location\"] == climate2[\"location\"]):\n",
    "                    print('joining....')\n",
    "                    \n",
    "                    join_data = join_data_cli_sat(climate2,satData_3)\n",
    "                    clim_col.insert(join_data)\n",
    "                    \n",
    "                    #insert the data into the mongo with handling the exceptions : duplicate\n",
    "                    sat_col.update(satData_3, satData_3, upsert=True)\n",
    "                    sat_col.update(satData_2, satData_2, upsert=True)\n",
    "#                \n",
    "                    #check location\n",
    "                    close_location_2(join_data,satData_2)\n",
    "                \n",
    "                #if nothing to merge\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    print('no join')\n",
    "\n",
    "                    #check location\n",
    "                    close_location(climate2,satData_2)\n",
    "                    close_location(climate2,satData_3)\n",
    "                    close_location(satData_2,satData_3)\n",
    "                    clim_col.insert(climate2)\n",
    "                    \n",
    "                    #insert the data into the mongo with handling the exceptions\n",
    "                    sat_col.update(satData_3, satData_3, upsert=True)\n",
    "                    sat_col.update(satData_2, satData_2, upsert=True)\n",
    "                    \n",
    "\n",
    "\n",
    "               \n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "          \n",
    "    client.close()\n",
    "\n",
    "\n",
    "    \n",
    "################################################  INITIATE THE STREAM ################################################\n",
    "n_secs = 10 # set batch to 10 seconds\n",
    "topic = 'TaskC'\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\") #set 2 processors\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'localhost:9092', \n",
    "                        'group.id':'taskC-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "    \n",
    "lines= kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "\n",
    "# this line print to check the data IDs has gone through the app for a specific time\n",
    "a = kafkaStream.map(lambda x:x[0])\n",
    "a.pprint()\n",
    "\n",
    "\n",
    "ssc.start()  \n",
    "\n",
    "# ssc.awaitTermination()\n",
    "\n",
    "# ssc.start()\n",
    "time.sleep(3000) # Run stream for 20 mins just to get the data for visualisation\n",
    "# # ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
